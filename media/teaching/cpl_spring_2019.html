<html>
<head>
    <link rel="stylesheet" href="/media/css/style.css">
    <title>JHU Computational Psycholinguistics: Spring 2019</title>
</head>
<body>

<div class="inset">
    <h1>JHU Computational Psycholinguistics: Spring 2019</h1>
    <p>Tuesdays and Thursdays, 10:30-11:45 am</p>
    <p>Krieger 111</p>

    <table>
        <tr><td>Instructor</td><td> <p><a href="http://tallinzen.net">Tal Linzen</a></p><p>tal.linzen@jhu.edu</p><p>Office hour: Wed 3-4 pm, Krieger 243</p></td></tr>
        <tr><td>Teaching assistant</td><td> <p>Suhas Arehalli</p><p>sarehal1@jhu.edu</p><p>Office hour: Thu 3-4 pm, Krieger 136</p><p>Section: Fri 2-3 pm</tr>
    </table>

    <h2>Course description</h2>
	
    <p>
How do we understand and produce sentences in a language we speak? How do we acquire the knowledge that underlies this ability? Computational psycholinguistics seeks to address these questions using a combination of two approaches: computational models, which aim to replicate the processes that take place in the human mind; and human experiments, which are designed to test those models. The perspective we will take in this class is that models and experimental paradigms from psycholinguistics do not only advance our understanding of the cognitive science, but can also help us advance artificial intelligence and language technologies. While research in computational psycholinguistics spans all levels of linguistic structure, from speech to discourse, the focus of this class will be at the level of the sentence (syntax and semantics).
    </p>

<p>At the end of this class, you are expected to be able to:</p>

<ul>
    <li> Identify the central experimental tools and paradigms used in psycholinguistics.</li>
    <li> Explain central empirical findings in human syntactic comprehension and acquisition, in areas such as prediction, priming, and coping with syntactic ambiguity.</li>
    <li> Use computational tools such as grammars, Bayesian models and recurrent neural networks to implement basic computational psycholinguistic models.</li>
    <li> Design and run simple psycholinguistic experiments on humans and neural networks.
</ul>

<p><b>Prerequisites:</b> I will assume you're familiar with probability theory (e.g., Bayes' law) and are comfortable with Python programming. Experience with neural networks would be helpful. I will also assume familiarity with basic concepts in linguistics.</p>

<h2>Course requirements</h2>

<p>Your responsibilities for the course are:</p>
<ul>
    <li>Attend the lectures <b>and participate regularly</b>.</li>
    <li>If anything isn't clear, ask questions in class, on Piazza or during our office hours.</li>
    <li>Complete the assigned readings prior to the class in which they are discussed, and write a half-page (single-spaced) commentary on <b>10</b> of the readings. Acceptable commentary topics include identifying an issue with the design of the experiment, proposing a follow-up experiment or discussing a potential challenge to authors' interpretation. Commentaries on the textbook chapters (SLP3 and Goldberg) are not allowed.</li>
    <li>Complete the four homework assignments.</li>
    <li>Write a final project (a substantial one for students enrolled in the graduate section, and a smaller one for students enrolled in the undergraduate section).</p>
</ul>

<p><b>Homework assignments:</b> There will be four homework assignments. These assignments will involve implementing computational models discussed in class. You have a budget of ten late days to be used at your discretion over the course of the semester, for any reason (e.g., illness); you do not need to ask for permission to use them. Use your late days wisely: once the budget has been exhausted, late assignments will receive a grade of 0.
</p>

<p><b>Commentary grading:</b>
We will use the follow three-point key to grade your commentaries:

<ul>
    <li><b>1</b> if you submitted a response, but did not convincingly demonstrate that you had read the article.</li>

    <li><b>2</b> if you adequately demonstrated that you have read <b>the entire</b> article (e.g., by identifying its main contribution), but have merely summarized or restated the results or theories stated in the article.</li>

    <li><b>3</b> if you have gone above and beyond summarizing the paper, providing criticisms, proposing follow-ups, synthesizing with other literature, providing new perspectives on the work, etc.</li>
</ul>

<p>Half point adjustments will be made to account for work that does not quite fit in each of the categories, either up or down. Note that when you present an idea, we want to see at least some justification for that idea. We will not grade you on some notion of the  "validity" or "reasonableness" of your justifications or proposals, but just don't say something like "they should try using 345-gram models" or "they should have used a different corpus" without stating why it's interesting or why it matters. 

Graduate Students will be graded out of 3 points, undergrads out of 2. That means that undergrads are not responsible for doing anything other than demonstrating that they have read and (at least vaguely) understood what's going on. You are more than welcome to get the 3rd point though - it is extra credit.

<p><b>Laptop policy:</b> Cognitive scientists have found that laptop use in the classroom can lead to lower test scores:</p>

<p>Raviza, S. M., Uitvlugt, M. G., &amp; Fenn, K. M. (2016). <a href="http://faculty.missouri.edu/segerti/1000/LaptopClassroomUse.pdf">Logged in and zoned out: How laptop Internet use relates to classroom learning.</a> Psychological Science, 28(2), 171&ndash;180.</p>

<p>We recommend that you avoid using your laptop in class, except for activities that are directly related to the class (e.g., following a Jupyter notebook).</p>

<p><b>Final project:</b> You will be expected to write a final project in groups of two. Interdisciplinary groups made up of students whose backgrounds complement each other are particularly encouraged.  The timeline for the project is:</p>

<ul>
    <li>February 14: Email Suhas with a tentative group. Explain how the the group members contribute intellectually diverse perspectives to the group.</li>
    <li>By March 7: Finalize groups and discuss a project idea with Tal (grads) or Suhas (undergrads).</li>
    <li>March 21: Email Tal (G) or Suhas (UG) a project description (by 6 pm). The project description should be up to two single-spaced pages long, and include (1) a short summary of the idea, (2) at least three research articles that are relevant to the project and a brief explanation why each of them is relevant, (3) a list of concrete steps you will undertake to complete the project, (4) and the division of labor between the group members.</li>
    <li>April 12: Progress report due (up to two single-spaced pages).</li>
    <li>April 26: Circulate a final report on the project to all of the students in the class.</li>
    <li>Class presentations in the final week of class (grads, potentially subset of undergrads).</li>
</ul>

<p><b>Piazza:</b> We will be using a Piazza site to make announcements and answer questions. Soon all enrolled students should receive an invitation to join the Piazza site. Alternatively, you can add yourself to the <a href="https://piazza.com/class/jqpponjoyv32qw">site</a>.

<p><b>Readings:</b> There is no required textbook. All of the readings will be available on Piazza.</p>

<p>Many of the readings are from the <a href="https://web.stanford.edu/~jurafsky/slp3/">draft third edition</a> of Jurafsky and Martin's textbook Speech and Language Processing. Page numbers and chapters refer to the September 23, 2018 version (these chapters are also available on Piazza). </p>

<p>An optional resource to supplement the readings is Jacob Eisenstein's <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">new NLP textbook</a> (also work in progress).</p>

<h2>Anxiety, Stress and Mental Health</h2>

<p>If you are struggling with anxiety, stress, depression or other mental health related concerns, please consider visiting the JHU Counseling Center. If you are concerned about a friend, please encourage that person to seek out their services. The Counseling Center is located at 3003 North Charles Street in Suite S-200 and can be reached at 410-516-8278 and <a href="http://studentaffairs.jhu.edu/counselingcenter">online</a>.

<h2>Ethics policy</h2>

The strength of the university depends on academic and personal integrity. In this course, you must be honest and truthful. Ethical violations include cheating on exams, plagiarism, reuse of assignments, improper use of the Internet and electronic devices, unauthorized collaboration, alteration of graded assignments, forgery and falsification, lying, facilitating academic dishonesty, and unfair competition. Please report any ethics violations you witness to the instructor. You may consult the associate dean of student affairs and/or the chairman of the Ethics Board beforehand. See also the guide on ``Academic Ethics for Undergraduates'' and the <a href="http://ethics.jhu.edu">Ethics Board Web site</a>. In particular:

<p><b>Do not cheat.</b> You are encouraged to talk with other students about the content of the course, and to use written material (e.g., the slides and books, external websites, articles, newspaper/magazine stories) as sources, but your written work must be original to you, with the exception of short quotes that are clearly indicated as such (see next paragraph).</p>

<p><b>Do not plagiarize.</b> If you quote directly from a book or other resource, please indicate this with quotes ("...") and a parenthesized citation after the quoted material; in any case, do not quote extensively from other sources. If you are simply paraphrasing a portion of a resource, leave off the quotes but keep the citation. Use a simple format for citations, for example: "human language syntax is not regular (Chomsky 1957: pages xxx-xxx)".</p>

<h2>Disability services</h2>

Any student with a disability who may need accommodations in this class should obtain an accommodation letter from Student Disability Services, studentdisabilityservices@jhu.edu, 385 Garland, (410) 516-4720. Please bring it to our attention as early as possible so we can do the best we can to accommodate your needs.

<h2>Course outline</h2>

<p>The topics and readings may change during the semester, depending on our rate of progress and interests.</p>

<h3>Probabilistic prediction</h3>

<ul>
    <li>Marta Kutas, Katherine DeLong &amp; Nathaniel Smith (2011). <a href="https://vorpus.org/papers/kutas-delong-smith-2011-chapter.pdf">A look around at what lies ahead: Prediction and predictability in language processing</a>. In <i>Predictions in the brain: Using our past to generate a future</i>.</li>
    <li>Nathaniel Smith &amp; Roger Levy (2013). <a href="https://vorpus.org/papers/smith-levy-2013-predictability-logarithmic.pdf">The effect of word predictability on reading time is logarithmic</a>. <i>Cognition</i>.</li>
    <li>SLP Chapter 3: <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">Language modeling with n-grams</a> (through 3.4, inclusive).</li>
    <li>(Probability refresher: Sharon Goldwater (2018), Basic probability theory.)
</ul>

<h3>Human parsing</h3>

<ul>
    <li>Lyn Frazier (1987). <a href="/media/cpl/frazier_1987.pdf">Sentence processing: A tutorial review</a>. In <i>Attention and performance 12: The psychology of reading</i>, pages 560&ndash;569.</li>
    <li>Ken McRae &amp; Kazunaga Matsuki (2013). Constraint-based models of sentence processing. In <i>Sentence processing</i>, pages 51&ndash;62.</li>
    <li>Lyn Frazier &amp; Keith Rayner (1982). <a href="https://www.sciencedirect.com/science/article/pii/0010028582900081">Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences</a>. <i>Cognitive Psychology</i>.</li>
    <li>John Trueswell, Michael Tanenhaus and Susan Garnsey (1994). <a href="http://www.psych.upenn.edu/~trueswel/Trueswell_Papers/Trueswell_Tanenhuas_Garnsey_1994.pdf">Semantic influences on parsing: Use of thematic role information in syntactic ambiguity resolution</a>. <i>Journal of Memory and Language</i>.</li>
</ul>

<h3>Knowledge of grammar</h3>

<ul>
    <li>Noam Chomsky (1957). Syntactic structures (pages 13-25).</li>
    <li>SLP Chapter 10: <a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">Formal Grammars of English</a>.</li>
    <li>Jey Han Lau, Alexander Clark &amp; Shalom Lappin (2017). <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12414">Grammaticality, acceptability, and probability: a probabilistic view of linguistic knowledge</a>. <i>Cognitive Science</i>.</li>
</ul>


<h3>Computational parsers</h3>

<ul>
    <li>SLP Chapter 11: <a href="https://web.stanford.edu/~jurafsky/slp3/11.pdf">Syntactic Parsing</a> (pages 1&ndash;9).</li>
    <li>SLP Chapter 12: <a href="https://web.stanford.edu/~jurafsky/slp3/12.pdf">Statistical Parsers</a> (pages 1&ndash;14).</li>
    <li>Andreas Stolcke (1995). <a href="www.aclweb.org/anthology/J95-2002">An efficient probabilistic context-Free parsing algorithm that computes prefix probabilities</a>. <i>Computational Linguistics</i>. (pages 165&ndash;176)</li>
</ul>

<h3>Probabilistic models of human parsing</h3>

<ul>
    <li>Dan Jurafsky (1996). <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog2002_1">A probabilistic model of lexical and syntactic access and disambiguation</a>. <i>Cognitive Science</i>. (sections 2.1-2.2 and 4.1-4.4). </li>
    <li>John Hale (2001). <a href="www.aclweb.org/anthology/N01-1021">A probabilistic Earley parser as a psycholinguistic model</a>. <i>NAACL</i>.</li>
</ul>

<h3>Word vector representations</h3>

<ul>
    <li><a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">SLP Chapter 6</a>: Vector semantics (skip 6.5-6.7).</li>
    <li>Aylin Caliskan, Joanna Bryson &amp; Arvind Narayanan (2017). <a href="http://science.sciencemag.org/content/sci/356/6334/183.full.pdf">Semantics derived automatically from language corpora contain human-like biases</a>. <i>Science</i>.</li>
    <li>Tal Linzen, Emmanuel Dupoux &amp; Benjamin Spector (2016). <a href="https://www.aclweb.org/anthology/S16-2001">Quantificational features in distributional word representations</a>. <i>Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics</i>.</li>
    <li>(Background reading for students unfamiliar with neural networks: <a href="https://www.morganclaypool.com/doi/pdf/10.2200/S00762ED1V01Y201703HLT037">Goldberg (2017)</a>, pages 11&ndash;62.)
</ul>

<h3>Syntax in neural networks</h3>
<ul>
    <li>Yoav Goldberg (2017). <a href="https://www.morganclaypool.com/doi/pdf/10.2200/S00762ED1V01Y201703HLT037">Neural network methods for natural language processing</a>. Pages 163&ndash;189.</li>
    <li>Chris Olah (2015). <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs">Understanding LSTM networks</a>.</li>
    <li>Tal Linzen, Emmanuel Dupoux &amp; Yoav Goldberg (2016). <a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00115">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</a>. Transactions of the Association for Computational Linguistics. <b>(Chenyu)</b></li>
    <li>Ethan Wilcox, Roger Levy, Takashi Morita &amp; Richard Futrell (2018). <a href="https://www.aclweb.org/anthology/W18-5423">What do RNN language models learn about filler-gap dependencies?</a>. BlackboxNLP 2018. <b>(Panayiota)</b></li>
    <li>Tom McCoy, Bob Frank &amp; Tal Linzen (2018). <a href="http://mindmodeling.org/cogsci2018/papers/0399/0399.pdf">Revisiting the poverty of the stimulus: Hierarchical generalization without a hierarchical bias in recurrent neural networks</a>. Proceedings of the Annual Conference of the Cognitive Science Society.</li>
</ul>

<h3>Pragmatics as inference</h3>

<ul>
    <li>Noah Goodman &amp; Michael Frank (2016). <a href="https://cocolab.stanford.edu/papers/GoodmanFrank2016-TICS.pdf">Pragmatic language interpretation as probabilistic inference</a>. <i>Trends in Cognitive Sciences</i>.</li>
    <li>Leon Bergen, Roger Levy &amp; Noah Goodman (2016). <a href="http://semprag.org/article/view/sp.9.20">Pragmatic reasoning through semantic inference.</a>, pages 6&ndash;14. <i>Semantics and Pragmatics.</i></li>
    <li>Justine Kao, Jean Wu, Leon Bergen &amp; Noah Goodman (2014). <a href="https://www.pnas.org/content/pnas/111/33/12002.full.pdf">Nonliteral understanding of number words</a>. <i>PNAS</i>.</li>
</ul>

<h3>Information, communication and the noisy channel</h3>
<ul>
    <li>Kyle Mahowald, Evelina Fedorenko, Steven Piantadosi &amp; Edward Gibson (2013). <a href="https://evlab.mit.edu/sites/default/files/documents/Mahowald_et_al_2013_Cognition.pdf">Info/information theory: Speakers choose shorter words in predictive contexts</a>. <i>Cognition</i>.</li>
    <li>Florian Jaeger (2010). <a href="https://www.hlp.rochester.edu/resources/workshop_materials/EVELIN12/Jaeger10UIDcomplementizers-offprint.pdf">Redundancy and reduction: Speakers manage syntactic information density</a> (pages 23&ndash;27). <i>Cognitive Psychology</i>.</li>
    <li>SLP Appendix B: <a href="https://web.stanford.edu/~jurafsky/slp3/B.pdf">Spelling correction and the noisy channel (pages 1-7)</a>.</li>
    <li>Edward Gibson, Leon Bergen &amp; Steven Piantadosi (2013). <a href="https://www.pnas.org/content/pnas/110/20/8051.full.pdf">Rational integration of noisy evidence and prior semantic expectations in sentence interpretation</a>. PNAS.</li>
    <li>Roger Levy, Klinton Bicknell, Tim Slattery &amp; Keith Rayner (2009). <a href="https://www.pnas.org/content/pnas/106/50/21086.full.pdf">Eye movement evidence that readers maintain and act on uncertainty about past linguistic input</a>. PNAS.</li>
</ul>
</h3>

<h3>Syntactic priming and adaptation</h3>

<ul>
    <li>Kristen Tooley &amp; Matthew Traxler (2010). <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-818X.2010.00249.x">Syntactic priming effects in comprehension: A critical review</a>. <i>Language and Linguistics Compass.</i></li>
    <li>Amit Dubey, Frank Keller &amp; Patrick Sturt (2006). <a href="https://aclweb.org/anthology/papers/P/P06/P06-1053/">Integrating syntactic priming into an incremental probabilistic parser, with an application to psycholinguistic modeling.</a> <i>ACL</i>.</li>
    <li>Marten van Schijndel &amp; Tal Linzen (2018). <a href="https://aclweb.org/anthology/papers/D/D18/D18-1499/">A neural model of adaptation in reading</a>. <i>EMNLP</i>.</li>
</ul>

<h3>Memory and sentence processing</h3>
<ul>
    <li>Daniel Grodner &amp; Edward Gibson (2005). <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog0000_7">Consequences of the serial nature of linguistic input for sentenial complexity</a>. <i>Cognitive Science</i>.</li>
    <li>Rick Lewis &amp; Shravan Vasishth (2005). <a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_25">An activation-based model of sentence processing as skilled memory retrieval</a>. <i>Cognitive Science</i>.</li>
    <li>Roger Levy (2013). <a href="idiom.ucsd.edu/~rlevy/papers/levy-2013-memory-and-surprisal-corrected.pdf">Memory and surprisal in sentence processing</a>. Pages 78&ndash;87 and 99&ndash;103. In <i>Sentence Processing</i>.</li>
</ul>

<h2>Grading</h2>

<p><b>Extra credit:</b> There will be no individual extra credit opportunities.</p>

<p><b>Undergraduate grade composition:</b> Homeworks: 40%; responses: 30%; project: 20%; participation: 10%.</p>

<p><b>Graduate grade composition:</b> Homeworks: 30%; responses: 20%; project: 40%; participation: 10%.</p>

<p><b>Letter grades:</b> We will use the following key to assign letter grades:</p>

<table>
    <tr> <td>Number </td><td>Letter</td></tr>
<tr> <td> 97&ndash;100</td><td>A+</td></tr>
<tr> <td>   93&ndash;96</td><td>A</td></tr>
<tr> <td>   90&ndash;92</td><td>A-</td></tr>
<tr> <td>   87&ndash;89</td><td>B+</td></tr>
<tr> <td>   83&ndash;86</td><td>B</td></tr>
<tr> <td>   80&ndash;82</td><td>B-</td></tr>
<tr> <td>   77&ndash;79</td><td>C+</td></tr>
<tr> <td>   73&ndash;76</td><td>C</td></tr>
<tr> <td>   70&ndash;72</td><td>C-</td></tr>
<tr> <td>   60&ndash;69</td><td>D</td></tr>
<tr> <td>   0&ndash;59</td><td>F</td></tr>
</table>

    <table>
        <tr></tr>
    </table>
</div>
</body>
</html>
