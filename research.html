
<head>
    <link rel="stylesheet" href="/media/css/style.css">
    <title>Tal Linzen</title>
</head>
<body>
    <p id="myname">Tal Linzen</p>
    <ul id="navbar">
          <li id="home"><a href="/" title="Home">Home</a></li>
          <li id="research"><a href="/research.html" title="Research">Research</a></li>
          <li id="teaching"><a href="/teaching.html" title="Teaching">Teaching</a></li>
          <li id="cv"><a href="/media/tal_linzen_cv.pdf" title="CV">CV</a></li>
    </ul>
<div class="small_inset">
    <h2>In progress</h2>
    <div class="references">
        <p>Grusha Prasad &amp; <b>Tal Linzen</b>. SPAWNing Structural Priming Predictions from a Cognitively Motivated Parser. [<a href="https://arxiv.org/abs/2403.07202">arXiv</a>]</p>
        <p>William Merrill, Zhaofeng Wu, Norihito Naka, Yoon Kim &amp; <b>Tal Linzen</b>. Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment. <i>ACL Findings</i>. [<a href="https://arxiv.org/abs/2402.13956">arXiv</a>]</p>
        <p>Matthew Mandelkern &amp; <b>Tal Linzen</b>. Do language models' words refer? <i>Computational Linguistics</i>. [<a href="https://arxiv.org/abs/2308.05576">arXiv</a>]<p>
        <p>Najoung Kim, <b>Tal Linzen</b> &amp; Paul Smolensky. Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models. [<a href="https://arxiv.org/abs/2212.10769">arXiv</a>]</p>
    </div>

    <h2>2024</h2>
    <div class="references">
        <p>Suhas Arehalli &amp; <b>Tal Linzen</b>.  Neural networks as cognitive models of the processing of syntactic constraints. <i>Open Mind</i>. [<a href="https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00137/120937/Neural-Networks-as-Cognitive-Models-of-the">link</a>] [<a href="/media/papers/arehalli_linzen_2024_open_mind.pdf">pdf</a>]</p>
        <p>Aaron Mueller, Albert Webson, Jackson Petty &amp; <b>Tal Linzen</b> (2024).  In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax. <i>NAACL</i>. [<a href="https://arxiv.org/abs/2311.07811">arXiv</a>]
        <p>Tiwalayo Eisape, MH Tessler, Ishita Dasgupta, Fei Sha, Sjoerd van Steenkiste &amp; <b>Tal Linzen</b> (2024). A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. <i>NAACL</i>. [<a href="https://arxiv.org/abs/2311.00445">arXiv</a>]</p>
        <p>Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette &amp; <b>Tal Linzen</b> (2024). The Impact of Depth on Compositional Generalization in Transformer Language Models. <i>NAACL</i>. [<a href="https://arxiv.org/abs/2310.19956">arXiv</a>]</p>
        <p>Kuan-Jung Huang, Suhas Arehalli, Mari Kugemoto, Christian Muxica, Grusha Prasad, Brian Dillon &amp; <b>Tal Linzen</b> (2024). Large-scale benchmark yields no evidence that language model surprisal explains syntactic disambiguation difficulty. <i>Journal of Memory and Language.</i> [<a href="https://doi.org/10.1016/j.jml.2024.104510">link</a>] [<a href="/media/papers/huang_et_al_2024_jml.pdf">pdf</a>]</p>
    </div>

    <h2>2023</h2>
    <div class="references">
        <p>Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, <b>Tal Linzen</b> &amp; Ryan Cotterell (2023). Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora. <i>Proceedings of the BabyLM Challenge.</i> [<a href="https://aclanthology.org/2023.conll-babylm.1/">link</a>]</p>
        <p>Sophie Hao &amp; <b>Tal Linzen</b> (2023). Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number. <i>Findings of EMNLP</i>. [<a href="https://arxiv.org/abs/2310.15151">arXiv</a>]
        <p>Bingzhi Li, Lucia Donatelli, Alexander Koller, <b>Tal Linzen</b>, Yuekun Yao &amp; Najoung Kim (2023). SLOG: A Structural Generalization Benchmark for Semantic Parsing. <i>EMNLP</i>. [<a href="https://arxiv.org/abs/2310.15040">arXiv</a>]</p>
        <p>William Timkey &amp; <b>Tal Linzen</b> (2023). A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing. <i>Findings of EMNLP</i>. [<a href="https://arxiv.org/abs/2310.16142">arXiv</a>]</p>
    <p>Aaron Mueller &amp; <b>Tal Linzen</b> (2023). How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases. <i>ACL</i>. [<a href="/media/papers/mueller_linzen_2023_acl.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2305.19905">arXiv</a>]</p>
    <p>Aditya Yedetore, <b>Tal Linzen</b>, Robert Frank &amp; R. Thomas McCoy (2023). How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech. <i>ACL</i>. [<a href="https://aclanthology.org/2023.acl-long.521/">link</a>] [<a href="/media/papers/yedetore_et_al_2023_acl.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2301.11462">arXiv</a>]</p>
    <p>R. Thomas McCoy, Paul Smolensky, <b>Tal Linzen</b>, Jianfeng Gao &amp; Asli Celikyilmaz (2023). How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN. <i>TACL</i>. [<a href="/media/papers/mccoy_et_al_2023_tacl.pdf"</a>pdf</a>] [<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00567/116616">link</a>] [<a href="https://arxiv.org/abs/2111.09509">arXiv</a>]</p>
    <p>Cara Leong &amp; <b>Tal Linzen</b> (2023). Language models can learn exceptions to syntactic rules. <i>Society for Computation in Linguistics.</i> [<a href="https://scholarworks.umass.edu/scil/vol6/iss1/13/">link</a>] [<a href="/media/papers/leong_linzen_2023_scil.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2306.05969">arXiv</a>]</p>
    <p>Anastasia Kobzeva, Suhas Arehalli, <b>Tal Linzen</b> &amp; Dave Kush (2023). Neural Networks Can Learn Patterns of Island-insensitivity in Norwegian. <i>Society for Computation in Linguistics.</i> [<a href="https://scholarworks.umass.edu/scil/vol6/iss1/17/">link</a>] [<a href="/media/papers/kobzeva_et_al_2023_scil.pdf">pdf</a>] [<a href="https://psyarxiv.com/4grqf/">PsyArXiv</a>]</p>
    </div>

    <h2>2022</h2>
    <div class="references">
        <p>William Merrill, Alex Warstadt &amp; <b>Tal Linzen</b> (2022). Entailment semantics can be extracted from an ideal language model. <i>CoNLL</i>. [<a href="/media/papers/merrill_warstadt_linzen_2022_conll.pdf">pdf</a>] [<a href="https://aclanthology.org/2022.conll-1.13/">link</a>] [<a href="https://arxiv.org/abs/2209.12407">arXiv</a>]</p>
        <p>Aaron Mueller, Yu Xia &amp; <b>Tal Linzen</b> (2022). Causal analysis of syntactic agreement neurons in multilingual language models. <i>CoNLL</i>. [<a href="/media/papers/mueller_xia_linzen_2022_conll.pdf">pdf</a>] [<a href="https://aclanthology.org/2022.conll-1.8/">link</a>] [<a href="https://arxiv.org/abs/2210.14328">arXiv</a>]</p>
        <p>Suhas Arehalli, Brian Dillon &amp; <b>Tal Linzen</b> (2022). Syntactic surprisal from neural models predicts, but underestimates, human processing difficulty from syntactic ambiguities. <i>CoNLL</i>. [<a href="/media/papers/arehalli_dillon_linzen_2022_conll.pdf">pdf</a>] [<a href="https://aclanthology.org/2022.conll-1.20/">link</a>] [<a href="https://arxiv.org/abs/2210.12187">arXiv</a>]</p>
        <p>Kristijan Armeni, Christopher Honey &amp; <b>Tal Linzen</b> (2022). Characterizing verbatim short-term memory in neural language models. <i>CoNLL</i>. [<a href="/media/papers/armeni_honey_linzen_2022_conll.pdf">pdf</a>] [<a href="https://aclanthology.org/2022.conll-1.28/">link</a>] [<a href="https://arxiv.org/abs/2210.13569">arXiv</a>] </p>
        <p>Nouha Dziri, Hannah Rashkin, <b>Tal Linzen</b> &amp; David Reitter (2022). Evaluating attribution in dialogue systems: the BEGIN benchmark. <i>TACL</i>. [<a href="/media/papers/dziri_rashkin_linzen_reitter_2022_tacl.pdf">pdf</a>] [<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00506/113023/Evaluating-Attribution-in-Dialogue-Systems-The">link</a>] [<a href="https://arxiv.org/abs/2105.00071">arXiv</a>]</p>
        <p>Anastasia Kobzeva, Suhas Arehalli, <b>Tal Linzen</b> &amp; Dave Kush (2022). LSTMs can learn basic wh- and relative clause dependencies in Norwegian. <i>Cognitive Science Society</i>. [<a href="https://psyarxiv.com/wjavp/">PsyArXiv</a>] [<a href="/media/papers/kobzeva_arehalli_linzen_kush_2022_cogsci.pdf">pdf</a>]</p> 
        <p>Sebastian Schuster &amp; <b>Tal Linzen</b> (2022). When a sentence does not introduce a discourse entity, Transformer-based models still often refer to it. <i>NAACL</i>. [<a href="/media/papers/schuster_linzen_2022_naacl.pdf">pdf</a>] [<a href="https://aclanthology.org/2022.naacl-main.71/">link</a>] [<a href="https://arxiv.org/abs/2205.03472">arXiv</a>]</p>
        <p>Linlu Qiu, Peter Shaw, Panupong Pasupat, Pawe≈Ç Krzysztof Nowak, <b>Tal Linzen</b>, Fei Sha, Kristina Toutanova (2022). Improving compositional generalization with latent structure and data augmentation. <i>NAACL</i>. [<a href="https://aclanthology.org/2022.naacl-main.323/">link</a>] [<a href="/media/papers/qiu_shaw_et_al_2022_naacl.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2112.07610">arXiv</a>]</p>
    <p>Aaron Mueller, Robert Frank, <b>Tal Linzen</b>, Luheng Wang &amp; Sebastian Schuster (2022). Coloring the blank slate: Pre-training imparts a hierarchical inductive bias to sequence-to-sequence models. <i>Findings of ACL</i>. [<a href="https://aclanthology.org/2022.findings-acl.106/">link</a>] [<a href="/media/papers/mueller_frank_linzen_wang_schuster_2022_findings_acl.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2203.09397">arXiv</a>]</p>
    <p>Thibault Sellam, Steve Yadlowsky, Jason Wei, Naomi Saphra, Alexander D'Amour, <b>Tal Linzen</b>, Jasmijn Bastings, Iulia Turc, Jacob Eisenstein, Dipanjan Das, Ian Tenney &amp; Ellie Pavlick (2022). The MultiBERTs: BERT reproductions for robustness analysis. <i>ICLR</i>. [<a href="https://arxiv.org/abs/2106.16163">arXiv</a>]</p>
    </div>

    <h2>2021</h2>
    <div class="references">
        <p>Grusha Prasad &amp; <b>Tal Linzen</b> (2021). Rapid syntactic adaptation in self-paced reading: detectable, but only with many participants. <i>Journal of Experimental Psychology: Learning, Memory, and Cognition</i>. [<a href="/media/papers/prasad_linzen_2021_jeplmc.pdf">pdf</a>] [<a href="https://psycnet.apa.org/fulltext/2021-81424-001.html">link</a>] [<a href="https://psyarxiv.com/9ptg4/">PsyArXiv</a>]</p>
        <p>Jason Wei, Dan Garrette, <b>Tal Linzen</b> &amp; Ellie Pavlick (2021). Frequency effects on syntactic rule learning in Transformers. <i>EMNLP</i>. [<a href="https://aclanthology.org/2021.emnlp-main.72/">link</a>] [<a href="/media/papers/wei_garrette_linzen_pavlick_2021_emnlp.pdf">pdf</a>]</p>
        <p>Alicia Parrish, William Huang, Omar Agha, Soo-Hwan Lee, Nikita Nangia, Alex Warstadt, Karmanya Aggarwal, Emily Allaway, <b>Tal Linzen</b> &amp; Samuel R. Bowman (2021). Does putting a linguist in the loop improve NLU data collection? <i>Findings of EMNLP</i>. [<a href="https://aclanthology.org/2021.findings-emnlp.421/">link</a>] [<a href="/media/papers/parrish_et_al_2021_findings_emnlp.pdf">pdf</a>]</p>
        <p>Alicia Parrish*, Sebastian Schuster*, Alex Warstadt*, Omar Agha, Soo-Hwan Lee, Zhuoye Zhao, Samuel R. Bowman &amp; <b>Tal Linzen</b> (2021). NOPE: A corpus of naturally-occurring presuppositions in English. <i>CoNLL.</i> [<a href="/media/papers/parrish_schuster_warstadt_et_al_2021_conll.pdf">pdf</a>] [<a href="https://aclanthology.org/2021.conll-1.28/">link</a>]</p>
        <p>Shauli Ravfogel*, Grusha Prasad*, <b>Tal Linzen</b> &amp; Yoav Goldberg (2021). Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction. <i>CoNLL</i>. [<a href="/media/papers/ravfogel_prasad_linzen_goldberg_2021_conll.pdf">pdf</a>] [<a href="https://aclanthology.org/2021.conll-1.15/">link</a>]</p>
        <p>Laura Aina &amp; <b>Tal Linzen</b> (2021). The language model understood the prompt was ambiguous: probing syntactic uncertainty through generation. <i>BlackboxNLP</i>. [<a href="/media/papers/aina_linzen_2021_blackboxnlp.pdf">pdf</a>] [<a href="https://aclanthology.org/2021.blackboxnlp-1.4/">link</a>]</p>
        <p>Matthew Finlayson, Aaron Mueller, Stuart Shieber, Sebastian Gehrmann, <b>Tal Linzen</b> &amp; Yonatan Belinkov (2021). Causal analysis of syntactic agreement mechanisms in neural language models. <i>ACL</i>. [<a href="https://aclanthology.org/2021.acl-long.144/">link</a>] [<a href="/media/papers/finlayson_et_al_2021_acl.pdf">pdf</a>]</p>
        <p>Marten van Schijndel &amp; <b>Tal Linzen</b> (2021). Single-stage prediction models do not explain the magnitude of syntactic disambiguation difficulty. <i>Cognitive Science</i>. [<a href="https://onlinelibrary.wiley.com/doi/10.1111/cogs.12988">link</a>] [<a href="/media/papers/van_schijndel_linzen_2021_cognitive_science.pdf">pdf</a>] </p>
    <p>Charles Lovering, Rohan Jha, <b>Tal Linzen</b> &amp; Ellie Pavlick (2021). Predicting inductive biases of pre-trained models. <i>ICLR</i>. [<a href="https://openreview.net/forum?id=mNtmhaDkAr">link</a>] [<a href="/media/papers/lovering_jha_linzen_pavlick_2021_iclr.pdf">pdf</a>] [<a href="/media/bib/lovering_jha_linzen_pavlick_2021_iclr.bib">bib</a>]</p>
    <p>Karl Mulligan, Robert Frank &amp; <b>Tal Linzen</b> (2021). Structure here, bias there: Hierarchical generalization by jointly learning syntactic transformations. <i>Society for Computation in Linguistics.</i> [<a href="/media/bib/mulligan_frank_linzen_2021_scil.bib">bib</a>] [<a href="https://scholarworks.umass.edu/scil/vol4/iss1/13/">link</a>] [<a href="/media/papers/mulligan_frank_linzen_2021_scil.pdf">pdf</a>]
    <p><b>Tal Linzen</b> &amp; Marco Baroni (2021). Syntactic structure from deep learning. <i>Annual Reviews of Linguistics</i>. [<a href="/media/bib/linzen_baroni_2021_annual_reviews_linguistics.bib">bib</a>] [<a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-linguistics-032020-051035">link</a>] [<a href="/media/papers/linzen_baroni_2021_annual_reviews_linguistics.pdf">pdf</a>]</p>
    </div>
        
    <h2>2020</h2>
    <div class="references">
        <p>Paul Soulos, R. Thomas McCoy, <b>Tal Linzen</b> &amp; Paul Smolensky (2020). Discovering the compositional structure of vector representations with role learning networks. <i>BlackboxNLP</i>. [<a href="https://www.aclweb.org/anthology/2020.blackboxnlp-1.23/">link</a>] [<a href="/media/papers/soulos_et_al_2020_blackboxnlp.pdf">pdf</a>]
        <p>R. Thomas McCoy, Junghyun Min &amp; <b>Tal Linzen</b> (2020). BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. <i>BlackboxNLP</i>. [<a href="https://www.aclweb.org/anthology/2020.blackboxnlp-1.21/">link</a>] [<a href="/media/papers/mccoy_min_linzen_2020_blackboxnlp.pdf">pdf</a>]</p>
        <p>Najoung Kim &amp; <b>Tal Linzen</b> (2020). COGS: A compositional generalization challenge based on semantic interpretation. <i>EMNLP</i>. [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.731/">link</a>] [<a href="/media/papers/kim_linzen_2020_emnlp.pdf">pdf</a>] [<a href="/media/bib/kim_linzen_2020_emnlp.bib">bib</a>]</p>
        <p>R. Thomas McCoy, Erin Grant, Paul Smolensky, Tom Griffiths &amp; <b>Tal Linzen</b> (2020). Universal linguistic inductive biases via meta-learning. <i>Cognitive Science Society</i>. [<a href="/media/bib/mccoy_et_al_2020_cogsci.bib">bib</a>] [<a href="https://cognitivesciencesociety.org/cogsci20/papers/0132/index.html">link</a>] [<a href="/media/papers/mccoy_grant_smolensky_griffiths_linzen_2020_cogsci.pdf">pdf</a>]</p>
        <p>Suhas Arehalli &amp; <b>Tal Linzen</b> (2020). Neural language models capture some, but not all, agreement attraction effects. <i>Cognitive Science Society</i>. [<a href="/media/bib/arehalli_linzen_2020_cogsci.bib">bib</a>] [<a href="https://cognitivesciencesociety.org/cogsci20/papers/0069/index.html">link</a>] [<a href="/media/papers/arehalli_linzen_2020_cogsci.pdf">pdf</a>]</p>
        <p>Naomi Havron, Camila Scaff, Maria Julia Carbajal, <b>Tal Linzen</b>, Axel Barrault &amp; Anne Christophe (2020). Priming syntactic ambiguity resolution in children and adults. <i>Language, Cognition and Neuroscience</i>. [<a href="https://doi.org/10.1080/23273798.2020.1797130">link</a>] [<a href="/media/papers/havron_et_al_2020_lcn.pdf">pdf</a>]</p>
        <p><b>Tal Linzen</b> (2020). How can we accelerate progress towards human-like linguistic generalization? <i>ACL</i>. [<a href="/media/papers/linzen_2020_acl.pdf">pdf</a>] [<a href="/media/bib/linzen_2020_acl.bib">bib</a>]</p>
        <p>Aaron Mueller, Garrett Nicolai, Panayiota Petrou-Zeniou, Natalia Talmina &amp; <b>Tal Linzen</b> (2020). Cross-linguistic syntactic evaluation of word prediction models. <i>ACL</i>. [<a href="https://www.aclweb.org/anthology/2020.acl-main.490/">link</a>] [<a href="/media/papers/mueller_et_al_2020_acl.pdf">pdf</a>] [<a href="/media/bib/mueller_et_al_2020_acl.bib">bib</a>]</p>
        <p>Junghyun Min, Richard T. McCoy, Dipanjan Das, Emily Pitler &amp; <b>Tal Linzen</b> (2020). Syntactic data augmentation increases robustness to inference heuristics. <i>ACL</i>. [<a href="/media/papers/min_et_al_2020_acl.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2004.11999">arXiv</a>] [<a href="/media/bib/min_et_al_2020_acl.bib">bib</a>]</p>
        <p>Michael Lepori, <b>Tal Linzen</b> &amp; R. Thomas McCoy (2020). Representations of syntax [MASK] useful: Effects of constituency and dependency structure in recursive LSTMs. <i>ACL</i>. [<a href="http://dx.doi.org/10.18653/v1/2020.acl-main.303">link</a>] [<a href="/media/papers/lepori_linzen_mccoy_2020_acl.pdf">pdf</a>] [<a href="/media/bib/lepori_linzen_mccoy_2020_acl.bib">bib</a>]</p>
        <p>R. Thomas McCoy, Robert Frank &amp; <b>Tal Linzen</b> (2020). Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks. <i>Transactions of the Association for Computational Linguistics</i>, 8, 125&ndash;140. [<a href="https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00304">link</a>] [<a href="/media/papers/mccoy_frank_linzen_2020_tacl.pdf">pdf</a>] [<a href="/media/bib/mccoy_frank_linzen_2020_tacl.bib">bib</a>]</p>
        <p>Natalia Talmina &amp; <b>Tal Linzen</b> (2020). Neural network learning of the Russian genitive of negation: optionality and structure sensitivity. <i>Society for Computation in Linguistics (SCiL)</i>, 21. [<a href="https://scholarworks.umass.edu/scil/vol3/iss1/21/">link</a>] [<a href="/media/papers/talmina_linzen_2020_scil.pdf">pdf</a>] [<a href="/media/bib/talmina_linzen_2020_scil.bib">bib</a>]</p>
    </div>

    <h2>2019</h2>
    <div class="references">
        <p>Marten van Schijndel, Aaron Mueller &amp; <b>Tal Linzen</b> (2019). Quantity doesn't buy quality syntax with neural language models. <i>EMNLP</i>. [<a href="https://www.aclweb.org/anthology/D19-1592/">link</a>] [<a href="/media/papers/vanschijndel_mueller_linzen_2019_emnlp.pdf">pdf</a>] [<a href="/media/bib/vanschijndel_mueller_linzen_2019_emnlp.bib">bib</a>] </p>
        <p>Grusha Prasad, Marten van Schijndel &amp; <b>Tal Linzen</b> (2019). Using priming to uncover the organization of syntactic representations in neural language models. <i>CoNLL</i>. [<a href="/media/papers/prasad_vanschijndel_linzen_2019_conll.pdf">pdf</a>] [<a href="/media/bib/prasad_vanschijndel_linzen_2019_conll.bib">bib</a>]</p>
        <p>R. Thomas McCoy, Ellie Pavlick &amp; <b>Tal Linzen</b> (2019). Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. <i>ACL</i>. [<a href="https://www.aclweb.org/anthology/P19-1334/">link</a>] [<a href="/media/papers/mccoy_pavlick_linzen_2019_acl.pdf">pdf</a>] [<a href="/media/bib/mccoy_pavlick_linzen_2019_acl.bib">bib</a>]</p>
        <p>Afra Alishahi, Grzegorz Chrupa&#0322;a &amp; <b>Tal Linzen</b> (2019). Analyzing and interpreting neural networks for NLP: A report on the first BlackboxNLP workshop. <i>Journal of Natural Language Engineering, 25</i>(4), 543&ndash;557. [<a href="https://doi.org/10.1017/S135132491900024X">link</a>] [<a href="/media/papers/alishahi_chrupala_linzen_2019_jnle.pdf">pdf</a>] [<a href="/media/bib/alishahi_chrupala_linzen_2019_jnle.bib">bib</a>]</p>
        <p>Brenden Lake, <b>Tal Linzen</b> &amp; Marco Baroni (2019). Human few-shot learning of compositional instructions. <i>Cognitive Science Society</i>. [<a href="https://mindmodeling.org/cogsci2019/papers/0123/index.html">link</a>] [<a href="http://tallinzen.net/media/papers/lake_linzen_baroni_2019_cogsci.pdf">pdf</a>] [<a href="/media/bib/lake_linzen_baroni_2019_cogsci.bib">bib</a>]</p>
        <p>Shauli Ravfogel, Yoav Goldberg &amp; <b>Tal Linzen</b> (2019). Studying the inductive biases of RNNs with synthetic variations of natural languages. <i>NAACL</i>. [<a href="https://www.aclweb.org/anthology/papers/N/N19/N19-1356/">link</a>] [<a href="/media/papers/ravfogel_goldberg_linzen_2019_naacl.pdf">pdf</a>] [<a href="/media/bib/ravfogel_goldberg_linzen_2019_naacl.bib">bib</a>]</p>
        <p>Najoung Kim, Roma Patel, Adam Poliak, Alex Wang, Patrick Xia, R. Thomas McCoy, Ian Tenney, Alexis Ross, <b>Tal Linzen</b>, Benjamin Van Durme, Samuel R. Bowman, Ellie Pavlick (2019). Probing what different NLP tasks teach machines about function word comprehension. <i>Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</i>, pages 235&ndash;249. [<a href="https://www.aclweb.org/anthology/papers/S/S19/S19-1026/">link</a>] [<a href="/media/papers/kim_et_al_2019_starsem.pdf">pdf</a>] [<a href="/media/bib/kim_et_al_2019_starsem.bib">bib</a>]</p>
        <p>R. Thomas McCoy, <b>Tal Linzen</b>, Ewan Dunbar &amp; Paul Smolensky (2019). RNNs implicitly implement tensor product representations. <i>ICLR</i>. [<a href="https://arxiv.org/abs/1812.08718">arXiv</a>] [<a href="/media/papers/mccoy_linzen_dunbar_smolensky_2019_iclr.pdf">pdf</a>]
        <p><b>Tal Linzen</b> (2019). What can linguistics and deep learning contribute to each other? Response to Pater. <i>Language</i>. [<a href="https://doi.org/10.1353/lan.2019.0015">link</a>] [<a href="/media/papers/linzen_2019_language.pdf">pdf</a>] [<a href="/media/bib/linzen_2019_language.bib">bib</a>]
        <p>R. Thomas McCoy &amp; <b>Tal Linzen</b> (2019). Non-entailed subsequences as a challenge for natural language inference. <i>Society for Computation in Linguistics (SCiL)</i> (extended abstract). [<a href="/media/papers/mccoy_linzen_2019_scil.pdf">pdf</a>] [<a href="/media/bib/mccoy_linzen_2019_scil.bib">bib</a>]</p>
        <p>Marten van Schijndel &amp; <b>Tal Linzen</b> (2019). Can entropy explain successor surprisal effects in reading? <i>Society for Computation in Linguistics (SCiL)</i>. [<a href="/media/papers/vanschijndel_linzen_2019_scil.pdf">pdf</a>] [<a href="/media/bib/vanschijndel_linzen_2019_scil.bib">bib</a>]</p>
    </div>

    <h2>2018</h2>
    <div class="references">
        <p>Rebecca Marvin &amp; <b>Tal Linzen</b> (2018). Targeted syntactic evaluation of language models. <i>EMNLP</i>. [<a href="http://aclweb.org/anthology/papers/D/D18/D18-1151/">link</a>] [<a href="/media/papers/marvin_linzen_2018_emnlp.pdf">pdf</a>] [<a href="https://vimeo.com/305208737">video</a>] [<a href="/media/bib/marvin_linzen_2018_emnlp.bib">bib</a>]</p>
        <p>Marten van Schijndel &amp; <b>Tal Linzen</b> (2018). A neural model of adaptation in reading. <i>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018)</i>, pages 4704&ndash;4710. [<a href="https://aclweb.org/anthology/papers/D/D18/D18-1499/">link</a>] [<a href="/media/papers/vanschijndel_linzen_2018_emnlp.pdf">pdf</a>] [<a href="https://vimeo.com/306153668">video</a>] [<a href="/media/bib/vanschijndel_linzen_2018_emnlp.bib">bib</a>]</p>
        <p><b>Tal Linzen</b> &amp; Yohei Oseki (2018). The reliability of acceptability judgments across languages. <i>Glossa: a journal of general linguistics, 3</i>(1), 100. [<a href="https://www.glossa-journal.org/articles/10.5334/gjgl.528/">link</a>] [<a href="/media/papers/linzen_oseki_2018_glossa.pdf">pdf</a>] [<a href="/media/bib/linzen_oseki_2018_glossa.bib">bib</a>] [<a href="https://doi.org/10.5281/zenodo.3959214">data</a>]</p>
        <p>Laura Gwilliams, <b>Tal Linzen</b>, David Poeppel &amp; Alec Marantz (2018). In spoken word recognition the future predicts the past. <i>Journal of Neuroscience</i> 38(35), 7585&ndash;7599. [<a href="http://www.jneurosci.org/content/38/35/7585">link</a>] [<a href="/media/papers/gwilliams_linzen_poeppel_marantz_2018_jneurosci.pdf">pdf</a>] [<a href="/media/bib/gwilliams_linzen_poeppel_marantz_2018_jneurosci.bib">bib</a>]</p>
        <p><b>Tal Linzen</b> &amp; Brian Leonard (2018). Distinct patterns of syntactic agreement errors in recurrent networks and humans. <i>Proceedings of the 40th Annual Conference of the Cognitive Science Society</i>, pages 692&ndash;697. [<a href="http://mindmodeling.org/cogsci2018/papers/0147/index.html">link</a>] [<a href="/media/papers/linzen_leonard_2018_cogsci.pdf">pdf</a>] [<a href="/media/bib/linzen_leonard_2018_cogsci.bib">bib</a>]</p>
        <p>Marten van Schijndel &amp; <b>Tal Linzen</b> (2018). Modeling garden path effects without explicit hierarchical syntax. <i>Proceedings of the 40th Annual Conference of the Cognitive Science Society</i>, pages 2600&ndash;2605. [<a href="http://mindmodeling.org/cogsci2018/papers/0496/index.html">link</a>] [<a href="/media/papers/vanschijndel_linzen_2018_cogsci.pdf">pdf</a>] [<a href="/media/bib/vanschijndel_linzen_2018_cogsci.bib">bib</a>]</p>
        <p>R. Thomas McCoy, Robert Frank &amp; <b>Tal Linzen</b> (2018). Revisiting the poverty of the stimulus: hierarchical generalization without a hierarchical bias in recurrent neural networks. <i>Proceedings of the 40th Annual Conference of the Cognitive Science Society</i>, pages 2093&ndash;2098. [<a href="http://mindmodeling.org/cogsci2018/papers/0399/index.html">link</a>] [<a href="/media/papers/mccoy_frank_linzen_2018_cogsci.pdf">pdf</a>] [<a href="/media/bib/mccoy_frank_linzen_2018_cogsci.bib">bib</a>]</p>
        <p>Kristina Gulordava, Piotr Bojanowski, Edouard Grave, <b>Tal Linzen</b>, Marco Baroni (2018). Colorless green recurrent networks dream hierarchically. In <i>Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</i>, pages 1195&ndash;1205. [<a href="https://www.aclweb.org/anthology/papers/N/N18/N18-1108/">link</a>] [<a href="/media/papers/gulordava_et_al_2018_naacl.pdf">pdf</a>] [<a href="/media/bib/gulordava_et_al_2018_naacl.bib">bib</a>]</p> 
        <p>Laura Gwilliams, David Poeppel, Alec Marantz &amp; <b>Tal Linzen</b> (2018). Phonological (un)certainty weights lexical activation. <i>Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL)</i>, pages 29&ndash;34.  [<a href="https://www.aclweb.org/anthology/papers/W/W18/W18-0104/">link</a>] [<a href="/media/papers/gwilliams_poeppel_marantz_linzen_2018_cmcl.pdf">pdf</a>] [<a href="/media/bib/gwilliams_poeppel_marantz_linzen_2018_cmcl.bib">bib</a>]</p>
        <p>James White, ReneÃÅ Kager, <b>Tal Linzen</b>, Giorgos Markopoulos, Alexander Martin, Andrew Nevins, Sharon Peperkamp, Krisztina PolgaÃÅrdi, Nina Topintzi &amp; Ruben van de Vijver (2018). Preference for locality is affected by the prefix/suffix asymmetry: Evidence from artificial language learning. <i>Proceedings of the 48th Annual Meeting of the North East Linguistic Society (NELS)</i>, pages 207&ndash;220. [<a href="/media/papers/white_et_al_nels_48.pdf">pdf</a>]</p>
        <p>Itamar Kastner &amp; <b>Tal Linzen</b> (2018). A morphosyntactic inductive bias in artificial language learning. <i>Proceedings of the 48th Annual Meeting of the North East Linguistic Society (NELS)</i>, pages 81&ndash;90. [<a href="/media/papers/kastner_linzen_2018_nels.pdf">pdf</a>] [<a href="/media/bib/kastner_linzen_2018_nels.bib">bib</a>]</p>
    </div>

    <h2>2017</h2>
    <div class="references">
        <p><b>Tal Linzen</b> &amp; Gillian Gallagher (2017). Rapid generalization in phonotactic learning. <i>Laboratory Phonology: Journal of the Association for Laboratory Phonology</i> 8(1): 24. [<a href="https://www.journal-labphon.org/articles/10.5334/labphon.44/">link</a>] [<a href="/media/papers/linzen_gallagher_2017_labphon.pdf">pdf</a>] [<a href="/media/bib/linzen_gallagher_2017_labphon.bib">bib</a>]</p>
        <p>&Eacute;mile Enguehard, Yoav Goldberg &amp; <b>Tal Linzen</b> (2017). Exploring the Syntactic Abilities of RNNs with Multi-task Learning. <i>Proceedings of the SIGNLL Conference on Computational Natural Language Learning (CoNLL)</i>, pages 3&ndash;14. [<a href="https://www.aclweb.org/anthology/papers/K/K17/K17-1003/">link</a>] [<a href="/media/papers/enguehard_goldberg_linzen_2017_conll.pdf">pdf</a>] [<a href="/media/bib/enguehard_goldberg_linzen_2017_conll.bib">bib</a>]</p>
        <p><b>Tal Linzen</b>, Noam Siegelman &amp; Louisa Bogaerts (2017). Prediction and uncertainty in an artificial language. <i>Proceedings of the 39th Annual Conference of the Cognitive Science Society</i>, pages 2592&ndash;2597. [<a href="https://mindmodeling.org/cogsci2017/papers/0492/index.html">link</a>] [<a href="/media/papers/linzen_siegelman_bogaerts_2017_cogsci.pdf">pdf</a>] [<a href="/media/bib/linzen_siegelman_bogaerts_2017_cogsci.bib">bib</a>]</p>
        <p>Gael Le Godais, <b>Tal Linzen</b> &amp; Emmanuel Dupoux (2017). Comparing character-level neural language models using a lexical decision task. <i>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL): Volume 2, Short Papers</i>, pages 125&ndash;130. [<a href="https://www.aclweb.org/anthology/papers/E/E17/E17-2020/">link</a>] [<a href="/media/papers/legodais_linzen_dupoux_2017_eacl.pdf">pdf</a>] [<a href="/media/bib/legodais_linzen_dupoux_2017_eacl.bib">bib</a>] </p>
    </div>

    <h2>2016</h2>
    <div class="references">
       <p><b>Tal Linzen</b>, Emmanuel Dupoux &amp; Yoav Goldberg (2016). Assessing the ability of LSTMs to learn syntax-sensitive dependencies. <i>Transactions of the Association for Computational Linguistics</i> 4, 521&ndash;535. [<a href="https://transacl.org/ojs/index.php/tacl/article/view/972">link</a>] [<a href="/media/papers/linzen_dupoux_goldberg_2016_tacl.pdf">pdf</a>] [<a href="/media/bib/linzen_dupoux_goldberg_2016_tacl.bib">bib</a>] </p>

        <p>Allyson Ettinger &amp; <b>Tal Linzen</b> (2016). Evaluating vector space models using human semantic priming results. <i>Proceedings of the First Workshop on Evaluating Vector Space Representations for NLP</i>, 72&ndash;77. [<a href="http://aclweb.org/anthology/papers/W/W16/W16-2513/">link</a>] [<a href="/media/papers/ettinger_linzen_2016_repeval.pdf">pdf</a>] [<a href="/media/bib/ettinger_linzen_2016_repeval.bib">bib</a>]</p>

        <p><b>Tal Linzen</b> (2016). Issues in evaluating semantic spaces using word analogies. <i>Proceedings of the First Workshop on Evaluating Vector Space Representations for NLP</i>, 13&ndash;18. [<a href="https://aclweb.org/anthology/papers/W/W16/W16-2503/">link</a>] [<a href="/media/papers/linzen_2016_repeval.pdf">pdf</a>] [<a href="/media/bib/linzen_2016_repeval.bib">bib</a>]</p>

        <p><b>Tal Linzen</b>, Emmanuel Dupoux &amp; Benjamin Spector (2016). Quantificational features in distributional word representations. <i>Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016)</i>, 1&ndash;11. [<a href="https://www.aclweb.org/anthology/papers/S/S16/S16-2001/">link</a>] [<a href="/media/papers/linzen_dupoux_spector_2016_starsem.pdf">pdf</a>] [<a href="/media/bib/linzen_dupoux_spector_2016_starsem.bib">bib</a>] </p>

        <p>Einat Shetreet, <b>Tal Linzen</b> &amp; Naama Friedmann (2016). Against all odds: exhaustive activation in lexical access of verb complementation options. <i>Language, Cognition &amp; Neuroscience</i> 31(9), 1206&ndash;1214. [<a href="http://www.tandfonline.com/doi/full/10.1080/23273798.2016.1205203">link</a>] [<a href="/media/papers/shetreet_linzen_friedmann_2016_lcn.pdf">pdf</a>] [<a href="/media/bib/shetreet_linzen_friedmann_2016_lcn.bib">bib</a>]</p>

        <p><b>Tal Linzen</b> (2016). The diminishing role of inalienability in the Hebrew Possessive Dative. <i>Corpus Linguistics and Linguistic Theory</i> 12(2), 325&ndash;354. [<a href="http://dx.doi.org/10.1515/cllt-2015-0023">link</a>] [<a href="/media/papers/linzen_2016_cllt.pdf">pdf</a>] [<a href="/media/bib/linzen_2016_cllt.bib">bib</a>] </p>

        <p><b>Tal Linzen</b> &amp; Florian Jaeger (2016). Uncertainty and expectation in sentence processing: Evidence from subcategorization distributions. <i>Cognitive Science</i> 40(6), 1382&ndash;1411. [<a href="http://onlinelibrary.wiley.com/doi/10.1111/cogs.12274/abstract">link</a>] [<a href="/media/papers/linzen_jaeger_2016_cognitive_science.pdf">pdf</a>] [<a href="/media/bib/linzen_jaeger_2016_cognitive_science.bib">bib</a>] </p> 
    </div>

    <h2>2015</h2>
    <div class="references">

        <p>Joseph Fruchter*, <b>Tal Linzen</b>*, Masha Westerlund &amp; Alec Marantz (2015). Lexical preactivation in basic linguistic phrases. <i>Journal of Cognitive Neuroscience</i> 27(10), 1912&ndash;1935. (* indicates equal contribution.) [<a href="http://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_00822">link</a>] [<a href="/media/papers/fruchter_linzen_westerlund_marantz_2015_jocn.pdf">pdf</a>] [<a href="/media/bib/fruchter_linzen_westerlund_marantz_2015_jocn.bib">bib</a>]

        <p><b>Tal Linzen</b> &amp; Timothy O'Donnell (2015). A model of rapid phonotactic generalization.<i> Proceedings of Empirical Methods in Natural Language Processing (EMNLP 2015)</i>, 1126&ndash;1131. [<a href="https://aclweb.org/anthology/papers/D/D15/D15-1134/">link</a>] [<a href="/media/papers/linzen_odonnell_2015_emnlp.pdf">pdf</a>] [<a href="/media/bib/linzen_odonnell_2015_emnlp.bib">bib</a>]</p> 

        <p>Maria Gouskova &amp; <b>Tal Linzen</b> (2015). Morphological conditioning of phonological regularization. <i>The Linguistic Review</i> 32(3), 427&ndash;473. [<a href="http://www.degruyter.com/view/j/tlir.2015.32.issue-3/tlr-2014-0027/tlr-2014-0027.xml?format=INT">link</a>] [<a href="http://ling.auf.net/lingbuzz/002246">lingbuzz</a>] [<a href="/media/papers/gouskova_linzen_2015_tlr.pdf">pdf</a>] [<a href="/media/bib/gouskova_linzen_2015_tlr.bib">bib</a>]</p>

        <p>Mira Ariel, Elitzur Dattner, John Du Bois &amp; <b>Tal Linzen</b> (2015). Pronominal datives: The royal road to argument status. <i>Studies in Language</i> 39(2), 257&ndash;321. [<a href="http://www.jbe-platform.com/content/journals/10.1075/sl.39.2.01ari">link</a>] [<a href="/media/papers/ariel_dattner_dubois_linzen_2015_studies_in_language.pdf">pdf</a>] [<a href="/media/bib/ariel_dattner_dubois_linzen_2015_studies_in_language.bib">bib</a>]</p>

    </div>

    <h2>2014</h2>
    <div class="references">
        <p><b>Tal Linzen</b> &amp; Florian Jaeger (2014). Investigating the role of entropy in sentence processing. <i>Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics (CMCL)</i>, 10‚Äì18. [<a href="https://www.aclweb.org/anthology/papers/W/W14/W14-2002/">link</a>] [<a href="/media/papers/linzen_jaeger_2014_cmcl.pdf">pdf</a>] [<a href="/media/bib/linzen_jaeger_2014_cmcl.bib">bib</a>]</p> 

        <p><b>Tal Linzen</b> &amp; Gillian Gallagher (2014). The timecourse of generalization in phonotactic learning. <i>Proceedings of Phonology 2013</i>, ed. John Kingston, Claire Moore-Cantwell, Joe Pater, and Robert Staub. Washington, DC: Linguistic Society of America. [<a href="http://journals.linguisticsociety.org/proceedings/index.php/amphonology/article/view/18/13">link</a>] [<a href="/media/papers/linzen_gallagher_2014_amp.pdf">pdf</a>] [<a href="/media/bib/linzen_gallagher_2014_amp.bib">bib</a>]</p>

        <p><b>Tal Linzen</b> (2014). Parallels between cross-linguistic and language-internal variation in Hebrew possessive constructions. <i>Linguistics</i> 52(3), 759&ndash;792. [<a href="http://www.degruyter.com/view/j/ling.2014.52.issue-3/ling-2014-0007/ling-2014-0007.xml">link</a>] [<a href="/media/papers/linzen_2014_linguistics.pdf">pdf</a>] [<a href="/media/bib/linzen_2014_linguistics.bib">bib</a>]<p>

        <p>Allyson Ettinger, <b>Tal Linzen</b> &amp; Alec Marantz (2014). The role of morphology in phoneme prediction: Evidence from MEG. <i>Brain and Language</i> 129, 14&ndash;23. [<a href="http://dx.doi.org/10.1016/j.bandl.2013.11.004">link</a>] [<a href="/media/papers/ettinger_linzen_marantz_2014_brln.pdf">pdf</a>] [<a href="/media/bib/ettinger_linzen_marantz_2014_brln.bib">bib</a>]</p>
    </div>

    <h2>2013</h2>

    <div class="references">
        <p><b>Tal Linzen</b>, Alec Marantz &amp; Liina Pylkk&auml;nen (2013). Syntactic context effects in single word recognition: An MEG study. <i>The Mental Lexicon</i> 8(2), 117&ndash;139. [<a href=http://dx.doi.org/10.1075/ml.8.2.01lin>link</a>] [<a href="/media/papers/linzen_marantz_pylkkanen_2013_mental_lexicon.pdf">pdf</a>] [<a href="/media/bib/linzen_marantz_pylkkanen_2013_mental_lexicon.bib">bib</a>]</p>

        <p><b>Tal Linzen</b>, Sophia Kasyanenko &amp; Maria Gouskova (2013). Lexical and phonological variation in Russian prepositions. <i>Phonology</i> 30(3), 453&ndash;515. [<a href="http://ling.auf.net/lingbuzz/001859">lingbuzz</a>] [<a href="http://dx.doi.org/10.1017/S0952675713000225">link</a>] [<a href="/media/papers/linzen_kasyanenko_gouskova_2013_phonology.pdf">pdf</a>] [<a href="https://github.com/TalLinzen/russian-preps">code and data</a>] [<a href="/media/bib/linzen_kasyanenko_gouskova_2013_phonology.bib">bib</a>]</p>
    </div>

</div>
</body>
</html>
